{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import word2vec\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from opencc import OpenCC\n",
    "\n",
    "cc = OpenCC('s2t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_list    = []\n",
    "url_list   = []\n",
    "title_list = []\n",
    "text_list  = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(filename, word_list):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        stop_words = f.read().split('\\n')\n",
    "        \n",
    "    stop_words.append('\\n')\n",
    "    stop_words.append('\\n\\n')\n",
    "    stop_words.append('\\n\\n\\n')\n",
    "    stop_words.append('\\n\\n\\n\\n')\n",
    "    stop_words.append('\\\\')\n",
    "    stop_words.append('/')\n",
    "    stop_words.append(' ')\n",
    "    stop_words.append('n')\n",
    "    stop_words.append('{')\n",
    "    stop_words.append('}')\n",
    "    stop_words.append('－')\n",
    "    \n",
    "    new_list = []\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word not in stop_words:\n",
    "            new_list.append(word)\n",
    "    \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.__version__\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA/\n",
      "AB/\n",
      "AC/\n",
      "AD/\n",
      "AE/\n",
      "AF/\n",
      "AG/\n",
      "AH/\n",
      "AI/\n",
      "AJ/\n",
      "AK/\n",
      "AL/\n",
      "AM/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getText time:  1509.420420408249  s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.621 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jieba time:  5754.35732960701  s\n",
      "\n",
      "train time:  2048.6979258060455  s\n",
      "\n",
      "total time:  9312.475675821304  s\n",
      "\n",
      "[('微積分', 0.8398054838180542), ('數理', 0.7876218557357788), ('微積', 0.7788676023483276), ('算術', 0.778183102607727), ('幾何學', 0.7742082476615906), ('數論', 0.7613482475280762), ('分學', 0.7602763175964355), ('邏輯學', 0.740354597568512), ('數學及', 0.7386688590049744), ('代數學', 0.7361712455749512), ('代數', 0.7187226414680481), ('拓撲學', 0.7128528952598572), ('幾何', 0.7055417895317078), ('數學理論', 0.6944530010223389), ('函數論', 0.6941742897033691), ('純數學', 0.6912258863449097), ('統計學', 0.6903806328773499), ('數學家', 0.6895079016685486), ('語言學', 0.6890572309494019), ('電磁學', 0.6877110004425049)]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "file_prefix = ['AA/', 'AB/', 'AC/', 'AD/', 'AE/', 'AF/', 'AG/', 'AH/', 'AI/', 'AJ/', 'AK/', 'AL/', 'AM/']\n",
    "#file_prefix = ['AK/', 'AL/', 'AM/']\n",
    "\n",
    "\n",
    "v2_id_list    = []\n",
    "v2_url_list   = []\n",
    "v2_title_list = []\n",
    "v2_text_list  = []\n",
    "\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "for prefix in file_prefix:\n",
    "    print(prefix)\n",
    "    \n",
    "    for i in range(0, 100):\n",
    "        fn = 'D://NLP/wiki_zh_2019/wiki_zh/' + prefix + 'wiki_' + str(\"{:02}\".format(i))\n",
    "        \n",
    "        try:\n",
    "            fd = open(fn, \"r\", encoding=\"utf-8\")\n",
    "        except:\n",
    "            continue;\n",
    "                \n",
    "        lines = fd.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip('')\n",
    "            \n",
    "            try:\n",
    "                #id_value    = line.split(', ')[0].strip('{').split(' ')[1].strip('\"')\n",
    "                #url_value   = line.split(', ')[1].split(' ')[1].strip('\"')\n",
    "                #title_value = cc.convert(str(line.split(', ')[2].split(' ')[1].strip('\"')))\n",
    "                text_value  = cc.convert(str(line.split(', ')[3].split(': \"')[1].strip('\"\\'}\\n')))\n",
    "\n",
    "                #v2_id_list.append(id_value)\n",
    "                #v2_url_list.append(url_value)\n",
    "                #v2_title_list.append(title_value)\n",
    "                v2_text_list.append(text_value)\n",
    "            except:\n",
    "                continue;\n",
    "\n",
    "        fd.close()\n",
    "\n",
    "getText_t = time.time()\n",
    "print(\"\\ngetText time: \", getText_t - start_t, \" s\\n\")\n",
    "        \n",
    "v2_cut_text_list = []\n",
    "#print(text_list[0])\n",
    "\n",
    "for text in v2_text_list:\n",
    "    text = jieba.lcut(text)\n",
    "    text = remove_stop_words('stop.txt', text)\n",
    "    \n",
    "    v2_cut_text_list.append(text)\n",
    "\n",
    "jieba_t = time.time()\n",
    "print(\"jieba time: \", jieba_t - getText_t, \" s\\n\")\n",
    "\n",
    "\n",
    "v2_test_model = Word2Vec(v2_cut_text_list)\n",
    "\n",
    "end_t = time.time()\n",
    "print(\"train time: \", end_t - jieba_t, \" s\\n\")\n",
    "print(\"total time: \", end_t - start_t, \" s\\n\")\n",
    "\n",
    "similar_word = v2_test_model.wv.most_similar('數學', topn=20)\n",
    "\n",
    "\n",
    "print(similar_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_test_model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_model = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('神學', 0.8336424231529236), ('倫理學', 0.8162001967430115), ('認識論', 0.7966198921203613), ('現象學', 0.7948501110076904), ('邏輯學', 0.7925100922584534), ('社會學', 0.7924025654792786), ('辯證法', 0.7846160531044006), ('歷史學', 0.7782753705978394), ('哲學及', 0.7733948826789856), ('主義哲學', 0.7645076513290405), ('形上學', 0.7581338286399841), ('知識論', 0.7546933889389038), ('經院', 0.7510842680931091), ('修辭學', 0.7510505318641663), ('哲學家', 0.7467262744903564), ('哲學理論', 0.737459659576416), ('胡塞爾', 0.7256459593772888), ('本體論', 0.7240965366363525), ('思辨', 0.7206656336784363), ('思想', 0.7173801064491272)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = v2_model.wv.most_similar('哲學', topn=20)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
